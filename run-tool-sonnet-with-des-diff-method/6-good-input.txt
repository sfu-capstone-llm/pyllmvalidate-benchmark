
# Description

# PR

Add support for always tokenizing async/await as keywords
Fixes #593

I looked into this bug with @ambv and @carljm, and we reached the
conclusion was that it's not possible for the tokenizer to determine if
async/await is a keyword inside all possible generators without breaking
the grammar for older versions of Python.

Instead, we introduce a new tokenizer mode for Python 3.7+ that will
cause all async/await instances to get parsed as a reserved keyword,
which should fix async/await inside generators.

# Issue #593 - Black can't parse new Python 3.7 async generator syntax

Operating system: Linux but unlikely to be OS specific
Python version: 3.7.1
Black version: 18.9b0
Does also happen on master: Yes (as of 32eed7d)

The following code (extracted from test_asyncgen.py in CPython) is currently not handled by black:

def make_arange(n):
    # This syntax is legal starting with Python 3.7
    return (i * 2 for i in range(n) if await wrap(i))
Running black on it results in:

error: cannot format test_asyncgen.py: Cannot parse: 3:45:     return (i * 2 for i in range(n) if await wrap(i))
All done! ðŸ’¥ ðŸ’” ðŸ’¥
1 file failed to reformat.
As per the comment, this is new Python 3.7 syntax, so presumably "just" a case of needing to update the parser to reflect the grammar change.

# Diff

diff --git a/black.py b/black.py
index c96d205..c8aa30b 100644
--- a/black.py
+++ b/black.py
@@ -48,6 +48,7 @@ from blib2to3 import pygram, pytree
 from blib2to3.pgen2 import driver, token
 from blib2to3.pgen2.grammar import Grammar
 from blib2to3.pgen2.parse import ParseError
+from blib2to3.pgen2.tokenize import TokenizerConfig
 
 
 __version__ = "19.3b0"
@@ -136,19 +137,28 @@ class Feature(Enum):
     NUMERIC_UNDERSCORES = 3
     TRAILING_COMMA_IN_CALL = 4
     TRAILING_COMMA_IN_DEF = 5
+    # The following two feature-flags are mutually exclusive, and exactly one should be
+    # set for every version of python.
+    ASYNC_IS_VALID_IDENTIFIER = 6
+    ASYNC_IS_RESERVED_KEYWORD = 7
 
 
 VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
-    TargetVersion.PY27: set(),
-    TargetVersion.PY33: {Feature.UNICODE_LITERALS},
-    TargetVersion.PY34: {Feature.UNICODE_LITERALS},
-    TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},
+    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
+    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
+    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
+    TargetVersion.PY35: {
+        Feature.UNICODE_LITERALS,
+        Feature.TRAILING_COMMA_IN_CALL,
+        Feature.ASYNC_IS_VALID_IDENTIFIER,
+    },
     TargetVersion.PY36: {
         Feature.UNICODE_LITERALS,
         Feature.F_STRINGS,
         Feature.NUMERIC_UNDERSCORES,
         Feature.TRAILING_COMMA_IN_CALL,
         Feature.TRAILING_COMMA_IN_DEF,
+        Feature.ASYNC_IS_VALID_IDENTIFIER,
     },
     TargetVersion.PY37: {
         Feature.UNICODE_LITERALS,
@@ -156,6 +166,7 @@ VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
         Feature.NUMERIC_UNDERSCORES,
         Feature.TRAILING_COMMA_IN_CALL,
         Feature.TRAILING_COMMA_IN_DEF,
+        Feature.ASYNC_IS_RESERVED_KEYWORD,
     },
     TargetVersion.PY38: {
         Feature.UNICODE_LITERALS,
@@ -163,6 +174,7 @@ VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
         Feature.NUMERIC_UNDERSCORES,
         Feature.TRAILING_COMMA_IN_CALL,
         Feature.TRAILING_COMMA_IN_DEF,
+        Feature.ASYNC_IS_RESERVED_KEYWORD,
     },
 }
 
@@ -748,20 +760,62 @@ def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
         return tiow.read(), encoding, newline
 
 
-def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:
+@dataclass(frozen=True)
+class ParserConfig:
+    grammar: Grammar
+    tokenizer_config: TokenizerConfig = TokenizerConfig()
+
+
+def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
     if not target_versions:
         # No target_version specified, so try all grammars.
         return [
-            pygram.python_grammar_no_print_statement_no_exec_statement,
-            pygram.python_grammar_no_print_statement,
-            pygram.python_grammar,
+            # Python 3.7+
+            ParserConfig(
+                pygram.python_grammar_no_print_statement_no_exec_statement,
+                TokenizerConfig(async_is_reserved_keyword=True),
+            ),
+            # Python 3.0-3.6
+            ParserConfig(
+                pygram.python_grammar_no_print_statement_no_exec_statement,
+                TokenizerConfig(async_is_reserved_keyword=False),
+            ),
+            # Python 2.7 with future print_function import
+            ParserConfig(pygram.python_grammar_no_print_statement),
+            # Python 2.7
+            ParserConfig(pygram.python_grammar),
         ]
     elif all(version.is_python2() for version in target_versions):
         # Python 2-only code, so try Python 2 grammars.
-        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]
+        return [
+            # Python 2.7 with future print_function import
+            ParserConfig(pygram.python_grammar_no_print_statement),
+            # Python 2.7
+            ParserConfig(pygram.python_grammar),
+        ]
     else:
         # Python 3-compatible code, so only try Python 3 grammar.
-        return [pygram.python_grammar_no_print_statement_no_exec_statement]
+        configs = []
+        # If we have to parse both, try to parse async as a keyword first
+        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
+            # Python 3.7+
+            configs.append(
+                ParserConfig(
+                    pygram.python_grammar_no_print_statement_no_exec_statement,
+                    TokenizerConfig(async_is_reserved_keyword=True),
+                )
+            )
+        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
+            # Python 3.0-3.6
+            configs.append(
+                ParserConfig(
+                    pygram.python_grammar_no_print_statement_no_exec_statement,
+                    TokenizerConfig(async_is_reserved_keyword=False),
+                )
+            )
+        # At least one of the above branches must have been taken, because every Python
+        # version has exactly one of the two 'ASYNC_IS_*' flags
+        return configs
 
 
 def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
@@ -769,8 +823,12 @@ def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -
     if src_txt[-1:] != "\n":
         src_txt += "\n"
 
-    for grammar in get_grammars(set(target_versions)):
-        drv = driver.Driver(grammar, pytree.convert)
+    for parser_config in get_parser_configs(set(target_versions)):
+        drv = driver.Driver(
+            parser_config.grammar,
+            pytree.convert,
+            tokenizer_config=parser_config.tokenizer_config,
+        )
         try:
             result = drv.parse_string(src_txt, True)
             break
diff --git a/blib2to3/pgen2/driver.py b/blib2to3/pgen2/driver.py
index 63b60bb..e681b52 100644
--- a/blib2to3/pgen2/driver.py
+++ b/blib2to3/pgen2/driver.py
@@ -29,12 +29,19 @@ from . import grammar, parse, token, tokenize, pgen
 
 class Driver(object):
 
-    def __init__(self, grammar, convert=None, logger=None):
+    def __init__(
+        self,
+        grammar,
+        convert=None,
+        logger=None,
+        tokenizer_config=tokenize.TokenizerConfig(),
+    ):
         self.grammar = grammar
         if logger is None:
             logger = logging.getLogger(__name__)
         self.logger = logger
         self.convert = convert
+        self.tokenizer_config = tokenizer_config
 
     def parse_tokens(self, tokens, debug=False):
         """Parse a series of tokens and return the syntax tree."""
@@ -97,7 +104,7 @@ class Driver(object):
 
     def parse_stream_raw(self, stream, debug=False):
         """Parse a stream and return the syntax tree."""
-        tokens = tokenize.generate_tokens(stream.readline)
+        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
         return self.parse_tokens(tokens, debug)
 
     def parse_stream(self, stream, debug=False):
@@ -111,7 +118,10 @@ class Driver(object):
 
     def parse_string(self, text, debug=False):
         """Parse a string and return the syntax tree."""
-        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
+        tokens = tokenize.generate_tokens(
+            io.StringIO(text).readline,
+            config=self.tokenizer_config,
+        )
         return self.parse_tokens(tokens, debug)
 
     def _partially_consume_prefix(self, prefix, column):
diff --git a/blib2to3/pgen2/tokenize.py b/blib2to3/pgen2/tokenize.py
index 1f51ff0..43e1d59 100644
--- a/blib2to3/pgen2/tokenize.py
+++ b/blib2to3/pgen2/tokenize.py
@@ -31,6 +31,7 @@ __credits__ = \
 
 import re
 from codecs import BOM_UTF8, lookup
+from attr import dataclass
 from blib2to3.pgen2.token import *
 
 from . import token
@@ -137,6 +138,10 @@ single_quoted = (
 
 tabsize = 8
 
+@dataclass(frozen=True)
+class TokenizerConfig:
+    async_is_reserved_keyword: bool = False
+
 class TokenError(Exception): pass
 
 class StopTokenizing(Exception): pass
@@ -334,7 +339,7 @@ def untokenize(iterable):
     ut = Untokenizer()
     return ut.untokenize(iterable)
 
-def generate_tokens(readline):
+def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
     """
     The generate_tokens() generator requires one argument, readline, which
     must be a callable object which provides the same interface as the
@@ -356,6 +361,9 @@ def generate_tokens(readline):
     contline = None
     indents = [0]
 
+    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
+    # `await` as keywords.
+    async_is_reserved_keyword = config.async_is_reserved_keyword
     # 'stashed' and 'async_*' are used for async/await parsing
     stashed = None
     async_def = False
@@ -506,7 +514,7 @@ def generate_tokens(readline):
                         yield (STRING, token, spos, epos, line)
                 elif initial.isidentifier():               # ordinary name
                     if token in ('async', 'await'):
-                        if async_def:
+                        if async_is_reserved_keyword or async_def:
                             yield (ASYNC if token == 'async' else AWAIT,
                                    token, spos, epos, line)
                             continue
diff --git a/tests/data/python37.py b/tests/data/python37.py
index 9781ff6..4401b7b 100644
--- a/tests/data/python37.py
+++ b/tests/data/python37.py
@@ -14,6 +14,14 @@ async def func():
                 self.async_inc, arange(8), batch_size=3
             )
         ]
+
+def awaited_generator_value(n):
+    return (await awaitable for awaitable in awaitable_list)
+
+def make_arange(n):
+    return (i * 2 for i in range(n) if await wrap(i))
+
+
 # output
 
 
@@ -39,3 +47,11 @@ async def func():
                 self.async_inc, arange(8), batch_size=3
             )
         ]
+
+
+def awaited_generator_value(n):
+    return (await awaitable for awaitable in awaitable_list)
+
+
+def make_arange(n):
+    return (i * 2 for i in range(n) if await wrap(i))


# Method Trace

blib2to3.pgen2.tokenize.any->blib2to3.pgen2.tokenize.group
blib2to3.pgen2.tokenize.maybe->blib2to3.pgen2.tokenize.group
blib2to3.pgen2.tokenize._combinations->blib2to3.pgen2.tokenize.<genexpr>
blib2to3.pygram.initialize->blib2to3.pygram.__init__
blib2to3.pygram.initialize->blib2to3.pgen2.grammar.copy
blib2to3.pygram.initialize->blib2to3.pgen2.driver.load_packaged_grammar
blib2to3.pgen2.driver.load_packaged_grammar->blib2to3.pgen2.driver._generate_pickle_name
blib2to3.pgen2.driver.load_packaged_grammar->blib2to3.pgen2.driver.load_grammar
blib2to3.pgen2.driver.load_packaged_grammar->genericpath.isfile
blib2to3.pgen2.driver._generate_pickle_name->posixpath.splitext
blib2to3.pgen2.driver._generate_pickle_name->posixpath.join
blib2to3.pgen2.driver._generate_pickle_name->posixpath.basename
blib2to3.pgen2.driver.load_grammar->logging.getLogger
blib2to3.pgen2.driver.load_grammar->blib2to3.pgen2.grammar.__init__
blib2to3.pgen2.driver.load_grammar->blib2to3.pgen2.grammar.load
blib2to3.pgen2.driver.load_grammar->blib2to3.pgen2.driver._newer
blib2to3.pgen2.driver._newer->genericpath.getmtime
blib2to3.pgen2.driver._newer->genericpath.exists
blib2to3.pgen2.grammar.copy->blib2to3.pgen2.grammar.__init__
black.dont_increase_indentation->typing.inner
black.dont_increase_indentation->functools.wraps
black.dont_increase_indentation->functools.update_wrapper
black.format_str->enum.__hash__
black.format_str->black.<setcomp>
black.format_str->black.lib2to3_parse
black.format_str->black.detect_target_versions
black.format_str->black.supports_feature
black.format_str->black.maybe_empty_lines
black.format_str->black.__str__
black.format_str->typing.__new__
black.format_str->black.split_line
black.format_str->.__init__
black.format_str->black.visit
black.format_str->black.normalize_fmt_off
black.format_str->black.get_future_imports
black.lib2to3_parse->blib2to3.pgen2.driver.__init__
black.lib2to3_parse->blib2to3.pgen2.tokenize.generate_tokens
black.lib2to3_parse->black.get_parser_configs
black.lib2to3_parse->blib2to3.pgen2.driver.parse_string
black.get_parser_configs->black.supports_feature
black.get_parser_configs->.__init__
black.get_parser_configs->black.<genexpr>
blib2to3.pgen2.driver.__init__->logging.getLogger
blib2to3.pgen2.driver.parse_string->blib2to3.pgen2.driver.parse_tokens
blib2to3.pgen2.driver.parse_tokens->blib2to3.pgen2.driver._partially_consume_prefix
blib2to3.pgen2.driver.parse_tokens->logging.debug
blib2to3.pgen2.driver.parse_tokens->blib2to3.pgen2.tokenize.generate_tokens
blib2to3.pgen2.driver.parse_tokens->blib2to3.pgen2.parse.setup
blib2to3.pgen2.driver.parse_tokens->blib2to3.pgen2.parse.__init__
blib2to3.pgen2.driver.parse_tokens->blib2to3.pgen2.parse.addtoken
blib2to3.pgen2.parse.addtoken->blib2to3.pgen2.parse.pop
blib2to3.pgen2.parse.addtoken->blib2to3.pgen2.parse.classify
blib2to3.pgen2.parse.addtoken->blib2to3.pgen2.parse.shift
blib2to3.pgen2.parse.addtoken->blib2to3.pgen2.parse.push
blib2to3.pgen2.parse.addtoken->blib2to3.pgen2.parse.__init__
blib2to3.pgen2.parse.shift->blib2to3.pytree.convert
blib2to3.pytree.convert->blib2to3.pytree.__new__
blib2to3.pytree.convert->blib2to3.pytree.__init__
blib2to3.pgen2.parse.pop->blib2to3.pytree.convert
blib2to3.pytree.__init__->blib2to3.pytree.invalidate_sibling_maps
black.get_future_imports->typing.inner
black.detect_target_versions->enum.__iter__
black.detect_target_versions->black.get_features_used
black.detect_target_versions->black.<setcomp>
black.get_features_used->blib2to3.pytree.pre_order
blib2to3.pytree.pre_order->blib2to3.pytree.pre_order
black.normalize_fmt_off->black.convert_one_fmt_off_pair
black.convert_one_fmt_off_pair->blib2to3.pytree.prefix
black.convert_one_fmt_off_pair->blib2to3.pytree.leaves
black.convert_one_fmt_off_pair->black.list_comments
blib2to3.pytree.leaves->blib2to3.pytree.leaves
black.supports_feature->black.<genexpr>
black.visit->black.visit_default
black.visit->black.visit_suite
black.visit->blib2to3.pytree.type_repr
black.visit->black.visit_DEDENT
black.visit->black.visit_ENDMARKER
black.visit->black.visit_async_stmt
black.visit->black.visit_INDENT
black.visit->black.visit_stmt
black.visit->black.visit_simple_stmt
blib2to3.pytree.type_repr->importlib._bootstrap.parent
black.visit_default->black.visit_default
black.visit_default->black.normalize_prefix
black.visit_default->black.any_open_brackets
black.visit_default->black.visit
black.visit_default->black.append
black.visit_default->black.generate_comments
black.visit_stmt->black.line
black.visit_stmt->black.normalize_invisible_parens
black.visit_stmt->black.visit
black.normalize_invisible_parens->black.is_one_tuple
black.normalize_invisible_parens->blib2to3.pytree.__new__
black.normalize_invisible_parens->blib2to3.pytree.remove
black.normalize_invisible_parens->blib2to3.pytree.__init__
black.normalize_invisible_parens->blib2to3.pytree.prefix
black.normalize_invisible_parens->blib2to3.pytree.insert_child
blib2to3.pytree.prefix->blib2to3.pytree.prefix
blib2to3.pytree.prefix->blib2to3.pytree.changed
black.line->black.__bool__
black.line->.__init__
black.generate_comments->blib2to3.pytree.prefix
black.generate_comments->black.list_comments
black.normalize_prefix->blib2to3.pytree.prefix
blib2to3.pytree.changed->blib2to3.pytree.changed
black.append->black.is_class_paren_empty
black.append->blib2to3.pytree.prefix
black.append->black.is_complex_subscript
black.append->black.maybe_remove_trailing_comma
black.append->black.whitespace
black.append->black.mark
black.append->black.append_comment
black.mark->black.maybe_decrement_after_lambda_arguments
black.mark->black.is_split_before_delimiter
black.mark->black.maybe_decrement_after_for_loop_variable
black.mark->black.is_split_after_delimiter
black.mark->black.maybe_increment_for_loop_variable
black.mark->black.maybe_increment_lambda_arguments
black.is_split_before_delimiter->black.is_vararg
black.is_split_before_delimiter->blib2to3.pytree.prev_sibling
black.is_complex_subscript->black.get_open_lsqb
black.whitespace->black.preceding_leaf
black.whitespace->blib2to3.pytree.prev_sibling
blib2to3.pytree.prev_sibling->blib2to3.pytree.update_sibling_maps
black.preceding_leaf->blib2to3.pytree.prev_sibling
black.is_class_paren_empty->black.__bool__
black.is_class_paren_empty->black.is_class
black.is_class->black.__bool__
black.visit_suite->black.visit_default
black.visit_INDENT->black.visit_default
black.visit_INDENT->black.line
black.maybe_empty_lines->black._maybe_empty_lines
black._maybe_empty_lines->black.is_decorator
black._maybe_empty_lines->blib2to3.pytree.prefix
black._maybe_empty_lines->black.is_import
black._maybe_empty_lines->black.is_def
black._maybe_empty_lines->black.__bool__
black._maybe_empty_lines->black._maybe_empty_lines_for_class_or_def
black._maybe_empty_lines->black.is_class
black.is_decorator->black.__bool__
black._maybe_empty_lines_for_class_or_def->black.is_comment
black._maybe_empty_lines_for_class_or_def->black.is_decorator
black.split_line->black.is_comment
black.split_line->black.contains_inner_type_comments
black.split_line->black.is_line_short_enough
black.split_line->black.__str__
black.__str__->blib2to3.pytree.prefix
black.__str__->click.termui.style
black.__str__->black.__bool__
black.__str__->blib2to3.pytree.__unicode__
blib2to3.pytree.__unicode__->blib2to3.pytree.prefix
black.is_line_short_enough->black.contains_standalone_comments
black.visit_simple_stmt->black.visit_default
black.visit_simple_stmt->black.line
black.visit_DEDENT->black.visit_default
black.visit_DEDENT->black.line
black.is_import->black.__bool__
black.is_import->black.is_import
blib2to3.pytree.remove->blib2to3.pytree.invalidate_sibling_maps
blib2to3.pytree.remove->blib2to3.pytree.changed
blib2to3.pytree.insert_child->blib2to3.pytree.invalidate_sibling_maps
blib2to3.pytree.insert_child->blib2to3.pytree.changed
black.visit_async_stmt->black.line
black.visit_async_stmt->black.visit
black.visit_ENDMARKER->black.visit_default
black.visit_ENDMARKER->black.line
black.assert_stable->black.format_str
black.read_pyproject_toml->toml.decoder.load
black.main->pathlib.is_file
black.main->pathlib.is_dir
black.main->enum.__hash__
black.main->pathlib.__new__
black.main->black.from_configuration
black.main->black.find_project_root
black.main->black.reformat_one
black.main->click.core.exit
black.main->pathlib.__hash__
black.main->black.__str__
black.main->black.re_compile_maybe_verbose
black.main->.__init__
black.main->black.return_code
black.main->click.termui.secho
black.re_compile_maybe_verbose->re.compile
black.find_project_root->pathlib.is_file
black.find_project_root->pathlib.is_dir
black.find_project_root->black.<genexpr>
black.find_project_root->collections.abc.__iter__
black.find_project_root->pathlib.parents
black.find_project_root->pathlib.__truediv__
black.reformat_one->pathlib.__eq__
black.reformat_one->pathlib.is_file
black.reformat_one->black.failed
black.reformat_one->pathlib.__hash__
black.reformat_one->black.done
black.reformat_one->black.read_cache
black.reformat_one->black.get_cache_info
black.reformat_one->pathlib.resolve
black.reformat_one->black.format_file_in_place
black.read_cache->pathlib.open
black.read_cache->pathlib.exists
black.read_cache->black.get_cache_file
black.read_cache->pathlib.__new__
black.read_cache->pathlib.__hash__
black.get_cache_file->black.get_cache_key
black.get_cache_file->pathlib.__truediv__
black.get_cache_key->black.<lambda>
black.get_cache_key->black.<genexpr>
black.get_cache_info->pathlib.stat
black.format_file_in_place->black.format_file_contents
black.format_file_in_place->black.decode_bytes
black.format_file_in_place->pathlib.stat
black.format_file_in_place->pathlib.suffix
black.format_file_in_place->pathlib.__fspath__
black.decode_bytes->codecs.__init__
black.decode_bytes->tokenize.detect_encoding
black.decode_bytes->codecs.decode
black.format_file_contents->black.format_str
black.failed->click.termui.secho
black.failed->pathlib.__str__
