## How to run

Go to a folder where you want to run the benchmarking:

```bash
mkdir benchmarking && cd benchmarking
```

Clone the BugsInPy repo:

```bash
git clone
```

Clone the benchmarking repo:

```bash
git clone
```

Clone the cli tool:

```bash
git clone
```

Run docker container:

```bash
docker run -it --rm -p 1234:1234 \
  -v "$HOME/benchmarking/BugsInPy:/workspace/BugsInPy" \
  -v "$HOME/benchmarking/pyllmvalidate-cli:/workspace/pyllmvalidate-cli" \
  -v "$HOME/benchmarking/pyllmvalidate-benchmark:/workspace/pyllmvalidate-benchmark" \
  python38-uv
```

Add a .env file with the OPENAI_API_KEY (required for mode #1 and #2).

## Modes

This tool has 3 different modes specified with the --mode flag:

1. output-context
2. run-tool
3. evaluation

Note: Mode #1 is not fully automated because the LLM may generate an invalid diff. Running the command again sometimes fixes it or you can fix the diff manually. Mode #1 and #2 requires an OPENAI_API_KEY.

### Mode 1: Output Context

The output-context clones the 23 good and 23 bad versions in the BugsInPy repo, installs the dependencies, generates the bad patch from the good patch, and all the other info require for the tool evaluation. The data used for the tool evaluation can be found in the pyllmvalidate-benchmark/output. The the cloned repos will be in the BugsInPy/frameworks/bin/temp/black-<bug-number>/<version>/black.

The data required to run the evaluation is provided in the output directory so you don't need to run this step if you want to reuse the data.

Note: This part isn't hands free because the LLM may not generate a corrent diff so you may need to rerun the command or fix it by hand.

To run:

```bash
uv run main.py --mode output-context
```

### Mode 2: Run tool

This run-tool mode uses the context and runs the tool. The default location for the output is in pyllmvalidate-benchmark/run-tool-output/. The tool returns correct or incorrent with the reason.

To run:

```bash
uv run main.py --mode run-tool
```

You can optionally specify the output folder with --output flag if you want to override the default location of pyllmvalidate-benchmark/run-tool-output

### Mode 3: Evalutation

The mode uses the output generated by the previous step to create the confusion matrix

To run:

```bash
uv run main.py --mode evaluation
```

You can optionally specify the output folder with --output flag if you want to override the default location of pyllmvalidate-benchmark/evaluation-data
